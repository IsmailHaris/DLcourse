{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Practical_Multi-Layer-Perceptron.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fabienMoutarde/DLcourse/blob/master/Practical_Multi_Layer_Perceptron.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7mfKRgPBmy4",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# Configuring and Training a Multi-layer Perceptron (MLP) in SciKit-Learn\n",
        "\n",
        "**(Notebook prepared by Pr Fabien MOUTARDE, Center for Robotics, MINES ParisTech, PSL Universit√© Paris)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dmokoz7MBmy7",
        "colab_type": "text"
      },
      "source": [
        "## 1. Understand and experiment MLP on a VERY simple classification problem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "fiDyCZK7Bmy8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###########################################################################################\n",
        "# Author: Pr Fabien MOUTARDE, Center for Robotics, MINES ParisTech, PSL Research University\n",
        "###########################################################################################\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing \n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import make_moons\n",
        "\n",
        "# Create artificial dataset (classification problem within 2 classes within R^2 input space)\n",
        "Xmoon, y_moon = make_moons(n_samples=900, noise=0.2, random_state=0)\n",
        "\n",
        "# Preprocess dataset, and split into training and test part\n",
        "Xmoon = StandardScaler().fit_transform(Xmoon)\n",
        "Xmoon_train, Xmoon_test, y_moon_train, y_moon_test = train_test_split(Xmoon, y_moon, test_size=0.7)\n",
        "\n",
        "# Encode class labels as binary vector (with exactly ONE bit set to 1, and all others to 0)\n",
        "Ymoon_train_OneHot = np.eye(2)[y_moon_train]\n",
        "Ymoon_test_OneHot = np.eye(2)[y_moon_test]\n",
        "\n",
        "# Print beginning of training dataset (for verification)\n",
        "print(\"Number of training examples = \", y_moon_train.size)\n",
        "print()\n",
        "print(\"  first \", round(y_moon_train.size/10), \"training examples\" )\n",
        "print(\"[  Input_features  ]     [Target_output]\")\n",
        "for i in range( int(round(y_moon_train.size/10) )):\n",
        "    print( Xmoon_train[i], Ymoon_train_OneHot[i])\n",
        "\n",
        "# Plot training+testing dataset\n",
        "################################\n",
        "cm = plt.cm.RdBu\n",
        "cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
        "\n",
        "# Plot the training points...\n",
        "plt.scatter(Xmoon_train[:, 0], Xmoon_train[:, 1], c=y_moon_train, cmap=cm_bright)\n",
        "#   ...and testing points\n",
        "plt.scatter(Xmoon_test[:, 0], Xmoon_test[:, 1], marker='x', c=y_moon_test, cmap=cm_bright, alpha=0.3)\n",
        "\n",
        "# Define limits/scale of plot axis\n",
        "x_min, x_max = Xmoon[:, 0].min() - .5, Xmoon[:, 0].max() + .5\n",
        "y_min, y_max = Xmoon[:, 1].min() - .5, Xmoon[:, 1].max() + .5\n",
        "plt.xlim(x_min, x_max)\n",
        "plt.ylim(y_min, y_max)\n",
        "\n",
        "plt.xticks(())\n",
        "plt.yticks(())\n",
        "\n",
        "# Actually render the plot\n",
        "print()\n",
        "print(\"PLOT OF TRAINING EXAMPLES AND TEST DATASET\")\n",
        "print(\"Datasets: circles=training, light-crosses=test [and red=class_1, blue=class_2]\")\n",
        "\n",
        "plt.ioff()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ul1sClnVBmzA",
        "colab_type": "text"
      },
      "source": [
        "**Building, training and evaluating a simple Neural Network classifier (Multi Layer Perceptron, MLP)**\n",
        "\n",
        "The SciKit-learn class for MLP is **MLPClassifier**.\n",
        "Please first read the [*MLPClassifier documentation*](http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifierMLPClassifier); to understand all parameters of the constructor.\n",
        "You can then begin by running the code block below, in which an initial set of hyper-parameter values has been chosen.\n",
        "**YOU MAY NEED TO CHANGE AT LEAST THE NUMBER OF HIDDEN NEURONS (and probably other hyper-parameters) IN ORDER TO BE ABLE TO LEARN A CORRECT CLASSIFIER**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "zNURqTk5BmzB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#########################################################\n",
        "# Create, fit and evaluate a MLP neural network classifier\n",
        "#########################################################\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# Create the MLP (with specific values for hyper-parameters)\n",
        "# NB: MANY HYPER-PARAMETER VALUES BELOW (in particular hidden_layer_sizes, solver, max_iter, etc...) \n",
        "#      HAVE VOLUNTARILY UNADAPTED VALUES, SO THAT YOU NEED TO FIND BY YOURSELF OPTIMIZED OR BETTER VALUES\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(1, ), activation='tanh', solver='sgd', \n",
        "                    alpha=0.0000001, batch_size=4, learning_rate='constant', learning_rate_init=0.005, \n",
        "                    power_t=0.5, max_iter=9, shuffle=True, random_state=11, tol=0.00001, \n",
        "                    verbose=True, warm_start=False, momentum=0.8, nesterovs_momentum=True, \n",
        "                    early_stopping=False, validation_fraction=0.2, \n",
        "                    beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
        "print(mlp)\n",
        "# NB about syntax for hidden layers: hidden_layer_sizes=(H1, ) means ONE hidden layer containing H1 neurons,\n",
        "#   while hidden_layer_sizes=(H1,H2, ) would mean TWO hidden layers of respective sizes H1 and H2\n",
        "# NB about iteration: max_iter specifies a number of EPOCHS (= going through all training examples)\n",
        "\n",
        "# Train the MLP classifier on the training dataset\n",
        "mlp.fit(Xmoon_train, Ymoon_train_OneHot)\n",
        "print()\n",
        "\n",
        "# Plot the LEARNING CURVE\n",
        "plt.title(\"Evolution of TRAINING ERROR during training\")\n",
        "plt.xlabel(\"Iterations (epochs)\")\n",
        "plt.ylabel(\"TRAINING ERROR\")\n",
        "plt.plot(mlp.loss_curve_)\n",
        "plt.show()\n",
        "\n",
        "# Evaluate acuracy on TEST data\n",
        "score = mlp.score(Xmoon_test,Ymoon_test_OneHot)\n",
        "print(\"Acuracy (on test set) = \", score)\n",
        "              "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggdeLAzJBmzE",
        "colab_type": "text"
      },
      "source": [
        "## Below, we visualize the learnt boundary between classes in (2D) input space ##\n",
        "\n",
        "**THIS SHOULD HELP YOU UNDERSTAND WHAT HAPPENS IF THERE ARE NOT ENOUGH HIDDEN NEURONS**\n",
        "\n",
        "Optional: add code that visualises on the same plot the straight lines corresponding to each hidden neuron (you will need to dig into MLPClassifier documentation to find the 2 input weights and the bias of each hidden neuron). YOU SHOULD NOTICE THAT THE CLASSIFICATION BOUNDARY IS SOME INTERPOLATION BETWEEN THOSE STRAIGHT LINES."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "amQEWkXeBmzF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot the decision boundary. For that, we will assign a color to each\n",
        "#   point in the mesh [x_min, x_max]x[y_min, y_max].\n",
        "\n",
        "h = .02  # Step size in the mesh\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                         np.arange(y_min, y_max, h))\n",
        "\n",
        "# Compute class probabilities for each mesh point\n",
        "Z = mlp.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
        "\n",
        "# Put the result into a color plot\n",
        "Z = Z.reshape(xx.shape)\n",
        "plt.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n",
        "\n",
        "# Plot also the training points\n",
        "plt.scatter(Xmoon_train[:, 0], Xmoon_train[:, 1], c=y_moon_train, cmap=cm_bright)\n",
        "# and testing points\n",
        "plt.scatter(Xmoon_test[:, 0], Xmoon_test[:, 1], marker='x', c=y_moon_test, cmap=cm_bright, alpha=0.3)\n",
        "\n",
        "# Axis ranges \n",
        "plt.xlim(xx.min(), xx.max())\n",
        "plt.ylim(yy.min(), yy.max())\n",
        "plt.xticks(())\n",
        "plt.yticks(())\n",
        "\n",
        "# Print acuracy on plot\n",
        "plt.text(xx.max() - .3, yy.min() + .3, ('%.2f' % score).lstrip('0'),\n",
        "                size=15, horizontalalignment='right')\n",
        "\n",
        "# Actually plot\n",
        "plt.ioff()\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "OGlitKTYBmzI",
        "colab_type": "text"
      },
      "source": [
        "Now, **check, by changing MLPClassifier parameters above and then rerunning training+eval+plots, the impact of main learning hyper-parameters:**\n",
        "- **number of neurons on hidden layer**: if very small, the classification boundary shall be too simple; if too large, overfitting might occur more easily. \n",
        "    **NB: generally, only ONE hidden layer is sufficient (cf. Cybenko theorem)**; *using more than one may require using ReLU as activation function, to avoid gradient \"vanishing\".*\n",
        "- **activation functions**\n",
        "- **number of iterations**: if too small, the training does not finish to converge; if too large, overfitting may occur. \n",
        "   **NB: it is therefore usually better to use \"early_stopping\" with quite large max_iter, so that the actual number of iterations shall adapt by STOPPING WHEN VALIDATION ERROR STOPS DECREASING**\n",
        "- **solver** (the best choice is generally 'adam'; for more details, see https://scikit-learn.org/stable/auto_examples/neural_networks/plot_mlp_training_curves.html#sphx-glr-auto-examples-neural-networks-plot-mlp-training-curves-py) \n",
        "- **learning_rate and momentum: the *initial learning rate* ALWAYS impacts training outcome a lot** (too small may stuck training in bad local minimum; too large can induce strong error fluctuations and possibly no convergence)\n",
        "- **impact of L2 weight regularization term (alpha)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "ndIsCcc4BmzJ",
        "colab_type": "text"
      },
      "source": [
        "### **Finally, use grid-search and cross-validation to find an optimized set of learning hyper-parameters (see code below).**\n",
        "\n",
        "**Because the values of learning hyper-parameters can DRASTICALLY change the outcome of training, it is ESSENTIAL THAT YOU ALWAYS TRY TO FIND OPTIMIZED VALUES FOR THE ALGORITHM HYPER-PARAMETERS. And this ABSOLUTELY NEEDS TO BE DONE USING \"VALIDATION\", either with a validation set separate from the training set, or using cross-validation. CROSS-VALIDATION is the MOST ROBUST WAY OF FINDING OPTIMIZED HYPER-PARAMETERS VALUES, and the GridSearchCV function of SciKit-Learn makes this rather straightforward.**\n",
        "\n",
        "**WARNING:** GridSearchCV launches many successive training sessions, so **can be rather long to execute if you compare too many combinations**. Note that in the example below, **instead of trying to optimize max_iter (the maximum number of training epochs), we choose to make it BIG ENOUGH (500 epochs) and to activate EARLY STOPPING (early_stopping=True)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "QHHkJECUBmzK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "param_grid = [\n",
        "  {'hidden_layer_sizes': [(5,), (10,), (20,), (40,)], \n",
        "   'learning_rate_init':[0.003, 0.01, 0.03, 0.1],\n",
        "   'alpha': [0.00001, 0.0001, 0.001]}\n",
        " ]\n",
        "print(param_grid)\n",
        "\n",
        "# Cross-validation grid-search (for finding best possible accuracy)\n",
        "clf = GridSearchCV( MLPClassifier(activation='tanh', alpha=1e-07, batch_size=4, beta_1=0.9,\n",
        "                                  beta_2=0.999, early_stopping=True, epsilon=1e-08,\n",
        "                                  hidden_layer_sizes=(10,), learning_rate='constant',\n",
        "                                  learning_rate_init=0.005, max_iter=500, momentum=0.8,\n",
        "                                  nesterovs_momentum=True, power_t=0.5, random_state=11, shuffle=True,\n",
        "                                  solver='adam', tol=1e-05, validation_fraction=0.3, verbose=False,\n",
        "                                  warm_start=False), \n",
        "                   param_grid, cv=3, scoring='accuracy') \n",
        "# NOTE THAT YOU CAN USE OTHER VALUE FOR cv (# of folds) and OTHER SCORING CRITERIA OTHER THAN 'accuracy'\n",
        "    \n",
        "clf.fit(Xmoon_train, Ymoon_train_OneHot)\n",
        "print(\"Best parameters set found on development set:\")\n",
        "print()\n",
        "print(clf.best_params_)\n",
        "print()\n",
        "print(\"Grid scores on development set:\")\n",
        "print()\n",
        "means = clf.cv_results_['mean_test_score']\n",
        "stds = clf.cv_results_['std_test_score']\n",
        "for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
        "    print(\"%0.3f (+/-%0.03f) for %r\"\n",
        "           % (mean, std * 2, params))\n",
        "print()\n",
        "print(\"Detailed classification report:\")\n",
        "print()\n",
        "print(\"The model is trained on the full development set.\")\n",
        "print(\"The scores are computed on the full evaluation set.\")\n",
        "print()\n",
        "y_true, y_pred = Ymoon_test_OneHot, clf.predict(Xmoon_test)\n",
        "print(classification_report(y_true, y_pred))\n",
        "print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7pCToNVBmzN",
        "colab_type": "text"
      },
      "source": [
        "## 2. WORK ON A REALISTIC DATASET:  A SIMPLIFIED HANDWRITTEN DIGITS DATASET\n",
        "\n",
        "**Please FIRST READ the [*Digits DATASET DESCRIPTION*](http://scikit-learn.org/stable/auto_examples/datasets/plot_digits_last_image.html#sphx-glr-auto-examples-datasets-plot-digits-last-image-py).**\n",
        "In this classification problem, there are 10 classes, with a total of 1797 examples (each one being a 64D vector corresponding to an 8x8 pixmap). \n",
        "\n",
        "**Assignment #1: find out which learning hyper-parameters should be modified in order to obtain a satisfying MLP digits classifier**\n",
        "\n",
        "**Assignment #2: modify the code below to use cross-validation and find best training hyper-parameters and MLP classifier you can for this handwritten digits classification task.**\n",
        "\n",
        "**Assignment #3: compute and plot the precision-recall curve (for each class).** NB: search into sciKit-learn documentation to find the function for that, and then add a code cell that uses it.\n",
        "\n",
        "**Assignment #4: display the confusion matrix as a prettier and more easily understable plot (cf. example on https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html)**\n",
        "\n",
        "**Assignment #5 (optional): plot the first layer of weights as images (see explanations and example code at http://scikit-learn.org/stable/auto_examples/neural_networks/plot_mnist_filters.html)**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "RNRmQO6JBmzO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "digits = load_digits()\n",
        "n_samples = len(digits.images)\n",
        "print(\"Number_of-examples = \", n_samples)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "print(\"\\n Plot of first example\")\n",
        "plt.gray() \n",
        "plt.matshow(digits.images[0]) \n",
        "print(\"CLOSE PLOT WINDOW TO CONTINUE\")\n",
        "plt.ioff()\n",
        "plt.show()\n",
        "\n",
        "# Flatten the images, to turn data in a (samples, feature) matrix:\n",
        "data = digits.images.reshape((n_samples, -1))\n",
        "\n",
        "Xdigits = data\n",
        "y_digits = digits.target\n",
        "Xdigits_train, Xdigits_test, y_digits_train, y_digits_test = train_test_split(Xdigits, y_digits, test_size=0.5)\n",
        "\n",
        "# NOTE THAT MANY HYPER-PARAMETER VALUES BELOW (in particular hidden_layer_sizes, solver, max_iter, etc...) \n",
        "#         HAVE VOLUNTARILY UNADAPTED VALUES, SO THAT YOU NEED TO FIND BY YOURSELF OPTIMIZED OR BETTER VALUES\n",
        "clf = MLPClassifier(hidden_layer_sizes=(10, ), activation='tanh', solver='sgd', \n",
        "                    alpha=0.00001, batch_size=4, learning_rate='constant', learning_rate_init=0.01, \n",
        "                    power_t=0.5, max_iter=9, shuffle=True, random_state=11, tol=0.00001, \n",
        "                    verbose=True, warm_start=False, momentum=0.8, nesterovs_momentum=True, \n",
        "                    early_stopping=False, validation_fraction=0.1, \n",
        "                    beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
        "print(clf)\n",
        "\n",
        "# Train the MLP classifier on training dataset\n",
        "clf.fit(Xdigits_train, y_digits_train)\n",
        "\n",
        "# Plot the LEARNING CURVE\n",
        "plt.title(\"Evolution of TRAINING ERROR during training\")\n",
        "plt.xlabel(\"Iterations (epochs)\")\n",
        "plt.ylabel(\"TRAINING ERROR\")\n",
        "plt.plot(mlp.loss_curve_)\n",
        "plt.show()\n",
        "\n",
        "# Evaluate acuracy on test data\n",
        "score = clf.score(Xdigits_test,y_digits_test)\n",
        "print(\"Acuracy (on test set) = \", score)\n",
        "y_true, y_pred = y_digits_test, clf.predict(Xdigits_test)\n",
        "print(classification_report(y_true, y_pred))\n",
        "\n",
        "# Display CONFUSION MATRIX on TEST set\n",
        "from sklearn.metrics import confusion_matrix\n",
        "print(\"CONFUSION MATRIX below\")\n",
        "confusion_matrix(y_true, y_pred)   \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xR9qdxWBmzQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
